\documentclass[11pt]{amsart}
\usepackage{calc,amssymb,amsthm,amsmath,fullpage    }
%\usepackage{mathtools}
\RequirePackage[dvipsnames,usenames, table]{xcolor}
\usepackage{hyperref}
\hypersetup{
bookmarks,
bookmarksdepth=3,
bookmarksopen,
bookmarksnumbered,
pdfstartview=FitH,
colorlinks,backref,hyperindex,
linkcolor=Sepia,
anchorcolor=BurntOrange,
citecolor=MidnightBlue,
citecolor=OliveGreen,
filecolor=BlueViolet,
menucolor=Yellow,
urlcolor=OliveGreen
}
\usepackage{xypic}
\usepackage{alltt}
\usepackage{multicol}  
%\usepackage{etex}
\usepackage{xspace}
\usepackage{rotating}
\interfootnotelinepenalty=100000

\usepackage{mabliautoref}
\usepackage{colonequals}
\frenchspacing
\input{kmacros3.sty}
\usepackage{stmaryrd}

\usepackage{verbatim}
\usepackage{enumerate}
\begin{document}
\title{{FastLinAlg} package for \emph{Macaulay2}}
\author{Boyana Martinova}
\email{u1056124@utah.edu}
\author{Marcus Robinson}
\email{robinson@math.utah.edu}
\author{Karl Schwede}
\email{schwede@math.utah.edu}
\author{Yuhui Yao}
\email{ywei018@gmail.com}
\address{Department of Mathematics, University of Utah, 155 S 1400 E Room 233, Salt Lake City, UT, 84112}
\date{\today}

\begin{abstract}
In this article, we present {\tt FastLinAlg.m2}, a package in \emph{Macaulay2} designed to
 introduce new methods focused on computations in function field linear algebra. 
Some key functionality that our 
package offers includes: finding a submatrix of a given rank in a provided matrix (when present), 
verifying that a ring is regular in codimension n, recursively computing the ideals of minors in a 
matrix, and finding an upper bound of the projective dimension of a module.
\end{abstract}

 
\keywords{FastLinAlg, Macaulay2}
\thanks{Martinova was supported by a University of Utah Mathematics REU fellowship and by the University of Utah ACCESS program.  
Robinson was supported by NSF RTG grant \#1840190.
Schwede was supported by NSF CAREER grant \#1501102 and NSF grant \#1801849.  
Yao was supported by a University of Utah Mathematics REU fellowship.}

\maketitle

\section{Introduction}

We start with some motivation.  
Suppose that $I = (f_1, \dots, f_m) \subseteq k[x_1, \dots, x_n]$ is a prime ideal.  The corresponding variety $X := V(I)$ is singular if and only if $I$ plus the ideal generated by the determinants of the $n-\dim X$ submatrices of the Jacobian matrix 
\[
\mathrm{Jac}(X) = \left( \begin{array}{c} {\partial f_i \over \partial x_j}\end{array}\right),
\]
generate the unit ideal.  
Unfortunately, even for relatively small values of $m$ and $n$, the number of such submatrices is prohibitive.  Suppose for instance that $n = 10, m = 15$ and $\dim X = 5$.  Then there are  
\[
  {10 \choose 5} \cdot {15 \choose 5} =  756756
\]
such submatrices.

We cannot reasonably compute all of their determinants (this computation is often time intensive).  This package attempts to fix this in several ways.  
\begin{enumerate}
  \item We try to compute just a portion of the determinants, in a relatively smart way.
  \item We offer some tools for computing determinants which are sometimes faster.
\end{enumerate}
Of course, computing the singular locus is not the only potential application. This technique has been applied to the related problem of showing that the singular locus has a certain codimension (for example, checking that a variety is R1 in order to prove normality).  We provide a function for giving a better upper bound on projective dimension of a non-homogeneous module.  Finally, this package has also been applied in the {\tt RationalMaps} Macaulay2 package.  

We provide the following functions:

\begin{itemize}
  \item{} {\tt getSubmatrixOfRank}, this tries to find a submatrix of a given rank, \autoref{sec.GetSubmatrixOfRank}.
  \item{} {{\tt isRankAtLeast}}, this uses {\tt getSubmatrixOfRank} to try to find lower bounds for the rank of a matrix, \autoref{sec.IsRankAtLeast}.
  \item{}  {\tt Rn}, tries to verify if an integral domain is regular in dimension n, \autoref{sec.Rn}.
  \item{}  {\tt projDim}, tries to find upper bounds for the projective dimension of a non-homogeneous module, 
  \autoref{sec.ProjDim}.
  \item{}  {\tt recursiveMinors}, computes the ideal of minors of a matrix via a recursive cofactor algorithm, as opposed to the included non-recursive cofactor algorithm, \autoref{sec.RecursiveMinors}.
\end{itemize}

The latest version of this package is available at:
\begin{center}
  {\tt https://github.com/kschwede/M2/blob/master/M2/Macaulay2/packages/FastLinAlg.m2}
\end{center}

%We include a final discussion of applications of this package to the {\tt RationalMaps} and {\tt SymbolicPowers} packages.

\subsection*{Acknowledgements:}  The authors thank David Eisenbud, Eloisa Grifo and Zhuang He for valuable conversations.

\section{Finding interesting submatrices}
\label{sec.FindingInterestingSubmatrices}

A lot of the speedups available in the package come down to finding interesting square submatrices of a given matrix. For example, it is often useful to compute a square submatrix whose determinant has small degree as this is less likely to vanish.

\subsection{How are the submatrices chosen?}

Consider the following matrix defined over $\mathbb{Q}[x,y]$.  
\[
  \left[\begin{array}{ccc}
    x & xy & 0 \\
    xy^2 & x^6 & 0 \\
    0 & x^2 y^3 & xy^4
  \end{array} \right]
\]
Suppose we want to choose a submatrix of size $2 \times 2$.  
Consider the monomial order {\tt Lex} where $x < y$.  We find, in the matrix, the nonzero element of smallest order.  
In this case, that is $x$.  We choose this element to be in part of our submatrix.
\[
  \left[\begin{array}{>{\columncolor{red!20}}ccc}
    \rowcolor{red!20}
    \cellcolor{red!40}{\bf x} & xy & 0 \\
    xy^2 & x^6 & 0 \\
    0 & x^2 y^3 & xy^4
  \end{array}\right]
\]
Hence we discard that row and column containing this term and find that the next smallest 
element with respect to our monomial order is $xy^4$.
\[
  \left[\begin{array}{cc}
    x^6 & 0 \\
    x^2 y^3 & x y^4
  \end{array}\right]
  \;\;\;\;
  \left[\begin{array}{c>{\columncolor{red!20}}c}    
    x^6 & 0 \\    
    \rowcolor{red!20}
    x^2 y^3 & \cellcolor{red!40} {\bf x y^4}
  \end{array}\right]
\]
Since we are only looking for a $2 \times 2$ submatrix, we stop here. 
We have selected the submatrix with rows $0$ and $2$ and columns $0$ and $2$. 
\[
  \left[\begin{array}{ccc}
    x &  0 \\
    0 & x y^4 \\
  \end{array} \right]
\]
The determinant of which is $x^2 y ^4$.  This happens to be the smallest $2 \times 2$ minor with respect to the given monomial order (which frequently happens, although it is certainly not always the case).

If we chose a different monomial order, we get a different submatrix, with a different determinant.  

For example, 
\begin{description}  
  \item[{\tt Lex}, $x > y$] We yield the submatrix with rows $0$ and $1$ and columns $0$ and $1$, whose determinant is $x^7 - x y^3$
  \item[{\tt GRevLex}, $x < y$] We yield the submatrix with rows $0$ and $2$ and columns $0$ and $1$, whose determinant is $x^3 y^3$
\end{description}
For any of these strategies, in this package we randomize the order of the variables before choosing a submatrix.

The strategies we implement in this package are a bit more complicated, however.  
If we have a matrix whose entries are not monomial, then we could reasonably either pick the 
submatrix of smallest entries with respect to our monomial order ({\tt LexSmallest} or {\tt GrevLexSmallest}), or the submatrix whose 
entries have the smallest terms ({\tt LexSmallestTerm} or {\tt GRevLexSmallestTerm}).  Both of these strategies are implemented.  

For example, consider the matrix
\[
  \left[\begin{array}{ccc}
    x^2 + y^2 & 0 & xy + 2x \\
    y^4 - x & 0 & 3 x^5 \\
    x^3 & x^4 y^5  - y^8 & 0
  \end{array} \right]
\]
In this case, if we are choosing the entries with smallest terms, we first replace each entry in the matrix 
with the smallest term.  For example, if we are using {\tt Lex} with $x < y$ we would obtain:
\[
  \left[\begin{array}{ccc}
    x^2 & 0 & 2x \\
    -x & 0 & 3 x^5 \\
    x^3 & x^4 y^5 & 0
  \end{array} \right]
\]
Then we proceed as before.  Notice that if there is a tie, it is broken randomly.  We also have the strategies {\tt GRevLexLargest} and {\tt LexLargest} available to the user.

Finally, we have two random strategies available.  {\tt Random} and {\tt RandomNonzero}.  
\begin{description}
  \item[{\tt Random}]  With this strategy, a random submatrix is chosen.
  \item[{\tt RandomNonzero}]  With this strategy, a random nonzero element is chosen in each step following the method used by the other strategies. This guarantees a submatrix where no row or column is zero which can be very useful when dealing with relatively sparse matrices.
\end{description}

\begin{remark}
  Different strategies work differently on different examples.  When you have a non-homogeneous matrix, with some entries that have constant terms, those entries will always be chosen first when picking smallest terms, regardless of the monomial order.  On the other hand, for homogeneous matrices, choosing the smallest term is frequently very effective.
\end{remark}



\subsubsection{Modifying the underlying matrix when using {\tt GRevLex*}}
Finally, when using {\tt GRevLex*} orders, we periodically change the underlying matrix 
by replacing terms of small order with terms of larger order in order to avoid computing the same submatrix.  For example, in the following matrix, after 
a couple iterations, we might replace the $x^2$ term with 
\[x^2 \cdot (\text{a random degree 1 polynomial}). \]
It might look something like the following.
\[
  \left[\begin{array}{ccc}
    x^2 & 0 & xy \\
    y^4 & 0 & x^5 \\
    x^3 & x^4 y^5 & 0
  \end{array} \right]
  \rightarrow
  \left[\begin{array}{ccc}
    x^2 (2x-7y) & 0 & xy \\
    y^4 & 0 & x^5 \\
    x^3 & x^4 y^5 & 0
  \end{array} \right].
\]
This forces the algorithm to make different choices to be made.  After several iterates are done, the matrix is reset again to its original form.



\subsection{The strategy options}  
The core features included in the package allow the user to choose which 
strategy should be used when selecting submatrices. This is done by setting a 
{\tt Strategy} option to one of the following.  
%{\tt GRevLexLargest, GRevLexSmallest,} and {\tt GRevLexSmallestTerm} use graded 
%reverse lexicographical order to find submatrices where each row and column have a large entry, small 
%entry, and entry with small term, respectively. {\tt LexLargest, LexSmallest,} and {\tt LexSmallestTerm} 
%use lexicographical order to select submatrices that fulfill the same criteria. {\tt Random} selects 
%submatrices at random.  

%Generally though, most functions will randomly use one of the strategies.  
\begin{itemize}
  \item {\tt StrategyDefault}: This strategy uses  
  {\tt LexSmallest, LexSmallestTerm, GRevLexSmallest, GRevLexSmallestTerm, Random,} and {\tt RandomNonzero} with equal probability.
  \item {\tt StrategyDefaultNonRandom}: This uses  
  {\tt LexSmallest, LexSmallestTerm, GRevLexSmallest, GRevLexSmallestTerm, Random,} and {\tt RandomNonzero} with equal probability.
  \item {\tt StrategyLexSmallest}: chooses 50\% of the submatrices using {\tt LexSmallest} and 50\% 
  using {\tt LexSmallestTerm}.
  \item {\tt StrategyGRevLexSmallest}: chooses 50\% of the submatrices using {\tt GRevLexSmallest} 
  and 50\% using {\tt GRevLexLargest}.
  \item {\tt StrategyRandom}: chooses submatrices by using 50\% {\tt Random} and 50\% {\tt RandonNonzero}.
  \end{itemize}  

The user can create their own custom strategy.  One creates a {\tt HashTable} which has the keys 
{\tt LexLargest, LexSmallestTerm, LexSmallest, GRevLexSmallestTerm, GrevLexSmallest, GRevLexLargest, Random, RandomNonzero} each with value an integer (the values need not sum to 100).  If one value is twice the size of another, that strategy will be employed twice as often.  For example, {\tt StrategyDefaultNonRandom} was created by the command:

{{\small\color{blue}
\begin{verbatim}
StrategyDefaultNonRandom = new HashTable from {
    LexLargest => 0,
    LexSmallestTerm => 25,
    LexSmallest => 25,
    GRevLexSmallestTerm => 25,
    GRevLexSmallest => 25,
    GRevLexLargest => 0,
    Random => 0,
    RandomNonzero => 0
};
\end{verbatim}}
}
\section{Find a submatrix of a given rank: {\tt getSubmatrixOfRank}}
\label{sec.GetSubmatrixOfRank}

This method examines the submatrices of an input matrix and attempts to find one of a 
given rank. If a submatrix with the specified rank is found, a list of two lists is returned. 
The first is the list of rows, the second is the list of columns, which describe the desired submatrix of desired rank. 
If no such submatrix is found, the function will return {\tt null}. 

\par The option {\tt MaxMinors} allows the user to control how many minors to consider. If left {\tt null}, 
the number considered is based on the size of the matrix. This method will choose the indicated amount of 
minors using one of the strategy options described below. If one of the chosen submatrices has the desired 
rank, the function will terminate and return its rows and columns. This process continues until a submatrix 
is found or {\tt MaxMinors} submatrices have been unsuccessfully checked.  The strategy can be controlled using the {\tt Strategy} option as described above, the default value is {\tt  StrategyDefaultNonRandom}.


\subsection{Examples of {\tt getSubmatrixOfRank}}
In the following example, we first create a $3 \times 4$ rational matrix, $M$. We execute two calls 
to {\tt getSubmatrixOfRank}, the first has no Strategy parameter and the second utilizes 
{\tt StrategyGRevLexSmallest}. Note that these calls return different indices, but both find 
valid rank $3$ submatrices. We then create a larger $9 \times 10$ matrix over the $3$-dimensional 
space of integers modulo $103$. We display the time needed for the {\tt rank} function to return, 
followed by the time elapsed during a call to {\tt getSubmatrixOfRank} when searching for a rank $7$
submatrix. We repeat these calculations on a new matrix with the same parameters, execting the call
to {\tt getSubmatrixOfRank} first, where we search for a rank $9$ submatrix, and then calling 
{\tt rank}. In both trials we found that {\tt getSubmatrixOfRank} significantly outperformed {\tt rank}. 

{\small\color{blue}
\begin{verbatim}
  i1 : loadPackage "FastLinAlg";

  i2 : R = QQ[x,y];
  
  i3 : M = random(R^{2,2,2},R^4)
  
  o3 = {-2} | x2+2/3xy+9/2y2    3/10x2+2/3xy+1/5y2 2x2+5/3xy+7/5y2    4/3x2+1/3xy+10/9y2 |
       {-2} | 3/2x2+2/3xy+2y2   1/2x2+3/2xy+3/4y2  6x2+5xy+4y2        9/5x2+1/5xy+7/2y2  |
       {-2} | 1/4x2+1/7xy+5/6y2 7/5x2+4xy+4/5y2    10/9x2+3/7xy+5/9y2 5/2x2+xy+7/6y2     |
               3       4
  o3 : Matrix R  <--- R
  
  i4 : getSubmatrixOfRank(3,M)
  
  o4 = {{2, 0, 1}, {0, 1, 3}}
  
  o4 : List
  
  i5 : getSubmatrixOfRank(3, M, Strategy=>StrategyGRevLexSmallest)
  
  o5 = {{0, 2, 1}, {1, 2, 0}}
  
  o5 : List

  i6 : Q = ZZ/103[x,y,z];
  
  i7 : N = random(Q^{7,7,7,7,7,7,7,8,8},Q^10); 
                9       10
  o7 : Matrix Q  <--- Q
  
  i8 : elapsedTime rank N;
       -- 17.701 seconds elapsed
  
  i9 : elapsedTime getSubmatrixOfRank(7,N);
       -- 0.0373561 seconds elapsed
  
  i10 : O = random(Q^{7,7,7,7,7,7,7,8,8},Q^10);
                9       10
  o10 : Matrix Q  <--- Q
  
  i11 : elapsedTime getSubmatrixOfRank(9,O);
  -- 0.76357 seconds elapsed
  
  i12 : elapsedTime rank O;
       -- 14.6581 seconds elapsed
  \end{verbatim}
}
In one of the core examples from the {\tt RationalMaps} package, before using this package a function would look at several thousands of submatrices (randomly) typically before finding a submatrix of the desired rank whereas this package finds one after looking at fewer than half a dozen.  Using this package sped up the computation of that example by more than one order of magnitude, see 
\cite[Page 7]{BottHassanzadehSchwedeSmolkinRationalMaps}, the non-maximal linear rank example.

\section{Finding lower bounds for matrix ranks: {\tt isRankAtLeast}}
\label{sec.IsRankAtLeast}

This method is a direct implementation of {\tt getSubmatrixOfRank}. This function returns a 
boolean indicating whether the rank of an input matrix, $M$, is greater than or equal to an 
input integer, $n$. In order to do so, the function first performs some basic checks 
to ensure a rank of $n$ is possible given $M$'s dimensions, then executes a call to 
{\tt getSubmatrixOfRank}. If {\tt getSubmatrixOfRank} returns a matrix, then this function 
will return true. However, if {\tt getSubmatrixOfRank} does not return a matrix, a conclusive 
answer can not be reached. As such, the method will then evaluate the rank of $M$ and return 
the appropriate boolean value. 

\par {\tt isRankAtLeast} is efficient when {\tt getSubmatrixOfRank} returns quickly, however may be 
costly if the results are inconclusive and a rank evaluation is necessary. As such, the 
described implementation is not optimized. In order to lead to time improvements, we developed a 
multithreaded version of this function that concurrrently evaluates the rank of $M$ and invokes 
{\tt getSubmatrixOfRank}. Once a thread has terminated, the method cancels the other and returns 
the appropriate value. During the implementation of this functionality, we discovered that 
\emph{Macaulay2} becomes unstable when cancelling threads and thus do currently not allow users 
to invoke the multithreaded version. However, this functionality is included in the package and can 
be made easily accessible once the stability issue is resolved. 

\subsection{Example of {\tt isRankAtLeast}}
The following example first creates and displays a smaller $3 \times 3$ matrix, $M$. We call
{\tt isRankAtLeast} to determine if its rank is at least 3. We then create a larger $9 \times 9$ 
matrix, $N$, and call {\tt isRankAtLeast} to determine if its rank is at least 7. Directly calling 
{\tt rank N} on a matrix of this size would take multiple seconds, whereas {\tt isRankAtLeast} 
returns in a fraction of the time. 

{\small\color{blue}
\begin{verbatim} 
  i1 : loadPackage "FastLinAlg";

  i2 : R = QQ[x,y,z];

  i3 : M = random(R^{2,2,2},R^3)

  o3 = {-2} | 7/2x2+2/3xy+y2+2/9xz+7/10yz+3/5z2     x2+5xy+1/2y2+2xz+7/9yz+7/5z2
      {-2} | 8/7x2+1/10xy+y2+2/3xz+3yz+1/2z2       3x2+7/8xy+1/9y2+3xz+10yz+5z2
      {-2} | 1/10x2+5/2xy+3/4y2+2/3xz+3/2yz+10/7z2 3x2+10/7xy+7/2y2+7/10xz+2yz+1/4z2
      ------------------------------------------------------------------------------
      6/7x2+9/7xy+3/10y2+2/5xz+2/5yz+5/4z2 |
      2/9x2+5/4xy+3y2+1/2xz+yz+7/8z2       |
      8/9x2+3/4xy+y2+10xz+9yz+4/5z2        |
              3       3
  o3 : Matrix R  <--- R

  i4 : isRankAtLeast(3,M)

  o4 = true

  ii5 : rank M

  oo5 = 3

  i6 : N = random(R^{6,6,6,6,6,6,6,7,7},R^9);
                9       9
  o6 : Matrix R  <--- R

  i7 : elapsedTime isRankAtLeast(7,N)
  -- 0.0654172 seconds elapsed

  o7 = true

\end{verbatim}
}

\section{Regular in codimension $n$: {\tt Rn}}
\label{sec.Rn}

Using the {\tt getSubmatrixOfRank} routines, we provide a function for checking if a variety is regular in codimension $n$, or $Rn$.  The default strategy is {\tt Strategy=>Default}.

The function {\tt Rn(ZZ, Ring)} returns {\tt true} if it verifies that the ring is regular in codimension $n$.  Note, this only works if the ring is equidimensional, as it is using a Jacobian criterion.  If it cannot make a determination, it returns {\tt null}.  If it ended up computing all minors of the matrix, and it still doesn't have the desired codimension, it will return {\tt false} (note this will likely only occur for small matrices).

\subsection{Example of {\tt Rn}}
We begin with an example of a 3 dimensional ring that is regular in codimension 1, but not in codimension 2.  It is generated by 12 equations in 7 variables.

{\small\color{blue}
\begin{verbatim}
i3 :  T = ZZ/101[x1,x2,x3,x4,x5,x6,x7];

i4 :  I =  ideal(x5*x6-x4*x7,x1*x6-x2*x7,x5^2-x1*x7,x4*x5-x2*x7,x4^2-x2*x6,x1*x4-x2*x5,
x2*x3^3*x5+3*x2*x3^2*x7+8*x2^2*x5+3*x3*x4*x7-8*x4*x7+x6*x7,x1*x3^3*x5+3*x1*x3^2*x7
+8*x1*x2*x5+3*x3*x5*x7-8*x5*x7+x7^2,x2*x3^3*x4+3*x2*x3^2*x6+8*x2^2*x4+3*x3*x4*x6
-8*x4*x6+x6^2,x2^2*x3^3+3*x2*x3^2*x4+8*x2^3+3*x2*x3*x6-8*x2*x6+x4*x6,x1*x2*x3^3
+3*x2*x3^2*x5+8*x1*x2^2+3*x2*x3*x7-8*x2*x7+x4*x7,x1^2*x3^3+3*x1*x3^2*x5+8*x1^2*x2
+3*x1*x3*x7-8*x1*x7+x5*x7);

o4 : Ideal of T

i5 : S = T/I; dim S

o6 = 3

i7 : time Rn(1, S)
     -- used 0.150734 seconds

o7 = true

i8 : time Rn(2, S)
     -- used 2.12777 seconds

i9 : time singularLocus S;
     -- used 8.29746 seconds

i10 : time dim o9
-- used 23.2483 seconds

o10 = 1
\end{verbatim}
}
You can see that the function {\tt Rn} verified that $S$ was regular in codimension 1 in a fraction of a second.  When we ran {\tt Rn(2, S)} nothing was returned, indicating that nothing was found.  Computing the jacobian ideal however took more than 8 seconds and verifying that it had dimension 1 took more than 23 seconds.  

\subsection{The {\tt Strategy} option}

Let us look in slightly more detail at the same example but this time using some different strategies.  For instance, you might think that it might be just as effective to choose random matrices, and sometimes it is. 
{\small\color{blue}
\begin{verbatim}
i11 :  time Rn(1, S, Strategy=>StrategyRandom, Verbose=>true)
  Rn: ring dimension =3, there are 17325 possible minors, we will compute up to 324 of them.
  Rn: About to enter loop
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 7, and computed = 7
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 9, and computed = 9
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 11, and computed = 11
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 14, and computed = 14
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 18, and computed = 18
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 24, and computed = 24
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 31, and computed = 31
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 40, and computed = 40
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 52, and computed = 52
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 67, and computed = 67
  Rn:  partial singular locus dimension computed, = 2
  Rn:  Loop step, about to compute dimension.  Submatrices considered: 87, and computed = 87
  Rn:  partial singular locus dimension computed, = 1
  Rn:  Loop completed, submatrices considered = 87, and computed = 87.  
       singular locus dimension appears to be = 1
       -- used 0.758961 seconds
  
o11 = true
\end{verbatim}
}
You can see that the {\tt StrategyRandom} option looked at 87 submatrices in this particular example.  Note it does not check to see if we have obtained the desired outcome with each submatrix considered, it does this periodically with the space between checks increasing.  The {\tt considered} values on each line tells how many submatrices have been selected.  Computed tells how many were not repeats (this will be very low with a random strategy).

Running {\tt Rn(1, S, Strategy=>StrategyRandom, Verbose=>true)} 50 times yielded:
\begin{enumerate}
  \item 61.3 average number of submatrices matrices considered.
  \item a median value of 40 or 52 sumatrices considered.
  \item a minimum value of 7 submatrices considered (one time).
  \item a maximum value of 248 submatrices considered (one time).
\end{enumerate}
Note 7 is the minimum number of submatrices that will be considered when checking regular in codimension 1 for this variety (the minimum number depends on the dimension).

On the other hand, the default strategy {\tt Rn(1, S, Strategy=>StrategyDefaultNonRandom, Verbose=>true)} run 50 times yields
\begin{enumerate}
  \item 12.1 average number of submatrices considered.
  \item a median value of 7 or 9 submatrices considered.
  \item a minimum value of 7 submatices considered (25 times).
  \item a maximum value of 40 submatrices considered (one time).
\end{enumerate}

In this particular example, {\tt Strategy=>StrategyLexSmallest} appears to be optimal.  Note that larger matrices tend to create even larger disparities.

\subsection{Notes on implementation}

As mentioned above, this function computes minors (based on the passed {\tt Strategy}) option until either it finds that the singular locus has the desired dimension, or until it has considered too many minors.  By default, it considers up to:
\[
  10\cdot (\text{Minimum number of minors needed}) + 8 \cdot \log_{1.3}(\text{possible minors}).
\]
This was simply chosen by experimentation.
Note that if you are trying to show your singular locus has a certain codimension, you will need a minimum number of minors.  The 10 multiplying it is because our default strategy uses multiple strategies, but only one might work well on a given matrix.  You can provide an alternate function of $x = (\text{minors needed})$ and $y = (\text{possible minors})$ passing it to the option {\tt MaxMinors}.  You can also pass {\tt MaxMinors} a number.

These matrices are considered in a loop.  We begin with computing a constant number of minors, by default $2 \cdot (\text{Minimum number of minors needed}) + 3$ and check whether the output has the right dimension.  You can provide a different function of $x = (\text{minors needed})$ via the option {\tt MinMinorsFunction}.
After that, we compute additional minors, checking periodically (based on an exponential function, $1.3^k$ minors considered before the next reset) whether our minors define a subset of the desired desired codimension.  You can provide your own function via the option {\tt CodimCheckFunction}.  If in this loop, a submatrix is considered again, it is not recomputed, but the counter is still increased.

\subsection{Other options}

This function also includes other options including the option {\tt ModP} which handles switching the coefficient field for a field of characteristic $p > 0$ (which you specify with {\tt ModP => p}. )

One can also control how determinants are computed with the {\tt DetStrategy} option, valid values are {\tt Bareiss}, {\tt Cofactor} and {\tt Recursive}.

\section{Projective dimension: {\tt projDim}}
\label{sec.ProjDim}

In April of 2019, it was pointed out in a thread on github
\begin{center}
  https://github.com/Macaulay2/M2/issues/936
\end{center}
that the command {\tt pdim} sometimes provides an incorrect value (an overestimate) for projective dimension for non-homogeneous modules over polynomial rings.  There it was also suggested that this could be addressed by looking at appropriate minors of the matrices in a possibly non-minimal resolution, but that in practice these matrices have too many minors to compute.  We have implemented a function {\tt projDim} that tries to address this by looking at only \emph{some} minors.  Our function does not solve the problem as it also gives only an upper bound on the projective dimension.  However, this upper bound is more frequently correct.

The idea is as follows.  Take a free resolution of a module $M$ over a polynomial ring $R$  
\[
  \xymatrix{
  0 & \ar[l] M  & \ar[l] F_0 & \ar[l]_{d_1} F_1 & \ar[l] \dots & \ar[l]_{d_{n-1}} F_{n-1}  & \ar[l]_{d_n} F_n & \ar[l] 0.  
  }
\] 
Each $d_i$ is given by a matrix.  The term $F_n$ is unnecessary (ie, $d_n$ splits) happens exactly when the $\rank F_n$ minors of $d_n$ generate the unit ideal.  In that case, we know or projective dimension is at most $n-1$.  However, we can continue in this way, we can compute the $(\rank F_{n-1} - \rank F_n)$-minors of $d_{n-1}$, and see if they generate the unit ideal.  Our algorithm of course only computes a subset of those minors.

\subsection{Example of {\tt projDim}}
In the below example, we take a monomial ideal of projective dimension 2, compute a non-homogeneous change of coordinates, and observe that {\tt pdim} returns an incorrect answer that {\tt projDim} corrects.

{\small\color{blue}
\begin{verbatim}
i1 : R = QQ[x,y,z,w];

i2 : I = ideal(x^4,x*y,w^3, y^4);

i3 : pdim module I

o3 = 2

i4 : f = map(R, R, {x+x^2+1, x+y+1, z+z^4+x-2, w+w^5+y+1});

i5 : pdim module f I

o5 = 3

i6 : time projDim module f I
    -- used 3.43851 seconds

o6 = 2

i7 : time projDim(module f I, MinDimension=>2)
    -- used 0.0503165 seconds

o7 = 2
\end{verbatim}
}

\subsection{Options}

As you can see in the previous example, setting {\tt MinDimension} will can substantially speed up the computation as it won't try to determine if the projective dimension is actually $1$.  

The option {\tt MaxMinors} can either be set to be a number (the number of a minors computed at each step).  Or it can be set to be a list of numbers (one for each step in the above algorithm).  Finally, it can be set to be a function of the dimension $d$ of the polynomial ring $R$ and the number $t$ of possible minors.  This is the default option, and the function is:  $5*d + 2*\log_{1.3}(t)$.
The option {\tt Strategy} is also available and it works as above with the default value being {\tt StrategyDefault}.



\section{Computing ideals of minors: {\tt recursiveMinors}}
\label{sec.RecursiveMinors}
\emph{Macaulay2} is a unique because it contains a {\tt minors} method that returns the ideal 
of minors of a certain size, $n$,  in a given matrix, a necessary step in 
locating singularities. However, the current implementation’s default is to 
evaluate determinants using the Bareiss algorithm, which is efficient when 
the entries in the matrix have a low degree and few variables, but very slow 
otherwise. The current minors method also allows users to compute determinants 
using cofactor expansion, but this strategy performs some unnecessary calculations, 
causing it to be quite costly as well. We improved the current cofactor 
expansion method to find the determinants of minors by adding recursion and 
multithreading throughout. We also eliminated said unnecessary calculations by 
ensuring that only the required determinants are being computed at each step 
of the recursion, rather than all possible determinants of the given size.

\par In order to do so, we programmed a method in \emph{Macaulay2}’s software that 
recursively finds all $n \times n$ minors by first computing the $2 \times 2$ minors 
and storing them in a hash table. Then we use the $2 \times 2$ minors to compute the 
necessary $3 \times 3$ minors, and so forth. This process is repeated recursively until 
the minors of size $n \times n$ are evaluated. At each step, we only compute the 
determinants that will be needed when performing a cofactor expansion on the 
following size minor.

\par To allow for further time improvements, we also utilized \emph{Macaulay2}’s existing 
parallel programming methods to multithread our code so different computations 
at each step of the recursion can occur simultaneously in separate threads. We 
divide the list of all determinants to be evaluated into different available 
threads and wait for them to finish before consolidating the results in a hash 
table and proceeding with the recursion. In order to more effectively utilize 
\emph{Macaulay2}'s multithreading methods, we also created a nanosleep method that 
waits a given number of nanoseconds, rather than full seconds. This function has already 
been incorporated into the software. 

\subsection{Example of {\tt recursiveMinors}}
\par Below, we first create a simple matrix, M, of one dimensional polynomials with 
rational coefficients and execute the {\tt recursiveMinors} method to find the ideal of all 
$3 \times 3$ minors. As can be seen, the result is equivalent to the output of the minors method 
when called with the same parameters. We then create a new, larger matrix, $N$, with two
dimensional rational coefficients and return the computation time for {\tt recursiveMinors} and 
minors utilizing both the Bareiss and Cofactor strategies. The {\tt recursiveMinors} method 
finished executing approximately six times faster than the Bareiss algorithm and almost seven
times faster than the Cofactor expansion, while yielding the same results. 

{\small\color{blue}
\begin{verbatim}
  i1 : loadPackage "FastLinAlg";

  i2 : allowableThreads = 8

  i3 : R = QQ[x];

  i4 : M = random(R^{2,2,2}, R^4)

  o4 = {-2} | x2    3x2   5/8x2 7/10x2 |
       {-2} | 3/4x2 2x2   7/4x2 9x2    |
       {-2} | x2    2/9x2 1/2x2 4/3x2  |
               3       4
  o4 : Matrix R  <--- R
  
  i5 : recursiveMinors(3,M)

              1403 6  449 6    292 6  517 6
  o5 = ideal (----x , ---x , - ---x , ---x )
               60     240       45    144
  
  o5 : Ideal of R
  
  i6 : recursiveMinors(3,M) == minors(3,M)
  
  o6 = true
  
  i7 : Q = QQ[x,y];

  i8 : N = random(Q^{5,5,5,5,5,5}, Q^7);
                6       7
  o8 : Matrix Q  <--- Q

  i9 : elapsedTime minors(5,N, Strategy => Bareiss);
  -- 3.0094 seconds elapsed
  
  o9 : Ideal of Q
  
  i10 : elapsedTime minors(5,N, Strategy => Cofactor);
  -- 3.76846 seconds elapsed
  
  o10 : Ideal of Q
  
  i11 : elapsedTime recursiveMinors(5,N);
  -- 0.590152 seconds elapsed;
  
  o11 : Ideal of Q
  
  i12 : recursiveMinors(5,N) == minors(5,N)
  
  o12 = true

\end{verbatim}
}


\bibliographystyle{skalpha}
\bibliography{MainBib}



\end{document}
